{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38574ba8",
   "metadata": {},
   "source": [
    "# Python Data Cleaning with Pandas and NumPy\n",
    "This notebook covers data cleaning techniques using Pandas and NumPy, as outlined in the [Real Python tutorial](https://realpython.com/python-data-cleaning-numpy-pandas/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8215024",
   "metadata": {},
   "source": [
    "Data scientists spend a large amount of their time cleaning datasets and getting them down to a form with which they can work. In fact, a lot of data scientists argue that the initial steps of obtaining and cleaning data constitute 80% of the job.\n",
    "\n",
    "Therefore, if you are just stepping into this field or planning to step into this field, it is important to be able to deal with messy data, whether that means missing values, inconsistent formatting, malformed records, or nonsensical outliers.\n",
    "\n",
    "In this tutorial, we’ll leverage Python’s [pandas](https://realpython.com/pandas-python-explore-dataset/) and NumPy libraries to clean data.\n",
    "\n",
    "We’ll cover the following:\n",
    "\n",
    "Dropping unnecessary columns in a DataFrame\n",
    "- Changing the index of a DataFrame\n",
    "- Using `.str()` methods to clean columns\n",
    "- Renaming columns to a more recognizable set of labels\n",
    "- Skipping unnecessary rows in a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77a300",
   "metadata": {},
   "source": [
    "Here are the datasets that we will be using:\n",
    "\n",
    "- [BL-Flickr-Images-Book.csv](https://github.com/realpython/python-data-cleaning/blob/master/Datasets/BL-Flickr-Images-Book.csv) – A CSV file containing information about books from the British Library\n",
    "- [olympics.csv](https://github.com/realpython/python-data-cleaning/blob/master/Datasets/olympics.csv) – A CSV file summarizing the participation of all countries in the Summer and Winter Olympics\n",
    "You can download the datasets from Real Python’s GitHub repository in order to follow the examples here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203514c",
   "metadata": {},
   "source": [
    "# British Library data\n",
    "## Importing Libraries\n",
    "First, we'll import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e0e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b01c2",
   "metadata": {},
   "source": [
    "## 1.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf49950",
   "metadata": {},
   "source": [
    "### 1.1 Dropping Columns in a DataFrame\n",
    "Often, you’ll find that not all the categories of data in a dataset are useful to you. For example, you might have a dataset containing student information (name, grade, standard, parents’ names, and address) but want to focus on analyzing student grades.\n",
    "\n",
    "In this case, the address or parents’ names categories are not important to you. Retaining these unneeded categories will take up unnecessary space and potentially also bog down runtime.\n",
    "\n",
    "pandas provides a handy way of removing unwanted columns or rows from a DataFrame with the `drop()` function. Let’s look at a simple example where we drop a number of columns from a DataFrame.\n",
    "\n",
    "First, let’s create a DataFrame out of the CSV file [BL-Flickr-Images-Book.csv]('https://raw.githubusercontent.com/realpython/python-data-cleaning/refs/heads/master/Datasets/BL-Flickr-Images-Book.csv'). Let's fetch the data straight from Github using `pd.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/realpython/python-data-cleaning/refs/heads/master/Datasets/BL-Flickr-Images-Book.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc60dbf",
   "metadata": {},
   "source": [
    "The first thing to do when loading a new dataset is to understand the size of the data we are dealing with. Let's compute the number of rows and columns (i.e., the \"shape\" of the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c5bf15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the shape of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e609143",
   "metadata": {},
   "source": [
    "When we look at the first five rows of the dataframe we see a lot of NaN values (\"not a number\"). Let's count how many NaNs there are in each column. If you don't know how to do this, use the resources available to you (Google, StackOverflow, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61451824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba73a7",
   "metadata": {},
   "source": [
    "When we look at the first five entries using the head() method, we can see that a handful of columns provide ancillary information that would be helpful to the library but isn’t very descriptive of the books themselves: Edition Statement, Corporate Author, Corporate Contributors, Former owner, Engraver, Issuance type and Shelfmarks.\n",
    "\n",
    "We can drop these columns in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd59911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Edition Statement',\n",
    "           'Corporate Author',\n",
    "           'Corporate Contributors',\n",
    "           'Former owner',\n",
    "           'Engraver',\n",
    "           'Contributors',\n",
    "           'Issuance type',\n",
    "           'Shelfmarks']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e73ee3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify we have dropped the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeedd46",
   "metadata": {},
   "source": [
    "Alternatively, we could also remove the columns by passing them to the columns parameter directly instead of separately specifying the labels to be removed and the axis where pandas should look for the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns using the column parameter of the drop method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbd490",
   "metadata": {},
   "source": [
    "This syntax is more intuitive and readable. What we’re trying to do here is directly apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d995106",
   "metadata": {},
   "source": [
    "### 1.2 Changing the Index of a DataFrame\n",
    "A pandas Index extends the functionality of NumPy arrays to allow for more versatile slicing and labeling. In many cases, it is helpful to use a uniquely valued identifying field of the data as its index.\n",
    "\n",
    "For example, in the dataset used in the previous section, it can be expected that when a librarian searches for a record, they may input the unique identifier (values in the Identifier column) for a book. Use `is_unique` on the Identifier column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the Identifier column contains only unique elements, using is_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c3a74",
   "metadata": {},
   "source": [
    "Let’s replace the existing index with this column using `set_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the index using .set_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3410b08",
   "metadata": {},
   "source": [
    "We can access each record in a straightforward way with `loc[]`. Although `loc[]` may not have all that intuitive of a name, it allows us to do label-based indexing, which is the labeling of a row or record without regard to its position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd568b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access index 206\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1d0b7",
   "metadata": {},
   "source": [
    "In other words, 206 is the first label of the index. To access it by position, we could use df.iloc[0], which does position-based indexing.\n",
    "\n",
    "Previously, our index was a RangeIndex: integers starting from 0, analogous to Python’s built-in range. By passing a column name to set_index, we have changed the index to the values in Identifier.\n",
    "\n",
    "You may have noticed that we reassigned the variable to the object returned by the method with df = df.set_index(...). This is because, by default, the method returns a modified copy of our object and does not make the changes directly to the object. We can avoid this by setting the inplace parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6420d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "153f71f2",
   "metadata": {},
   "source": [
    "### ✏️ Now it's your turn. \n",
    "> Try to replicate what we have done together. \n",
    "> 1. Drop the columns Edition Statement, Corporate Author, Corporate Contributors, Former owner, Engraver, Issuance type and Shelfmarks from the dataframe df.\n",
    "> 2. Change the index of the dataframe to the Identifier column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a43fef",
   "metadata": {},
   "source": [
    "### 1.3 Tidying up values and enforce data types\n",
    "So far, we have removed unnecessary columns and changed the index of our DataFrame to something more sensible. In this section, we will clean specific columns and get them to a uniform format to get a better understanding of the dataset and enforce consistency. In particular, we will be cleaning Date of Publication and Place of Publication.\n",
    "\n",
    "Upon inspection, all of the data types are currently the object dtype, which is roughly analogous to str in native Python.\n",
    "\n",
    "It encapsulates any field that can’t be neatly fit as numerical or categorical data. This makes sense since we’re working with data that is initially a bunch of messy strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436d9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place of Publication    object\n",
       "Date of Publication     object\n",
       "Publisher               object\n",
       "Title                   object\n",
       "Author                  object\n",
       "Flickr URL              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the type of each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71cd25",
   "metadata": {},
   "source": [
    "One field where it makes sense to enforce a numeric value is the date of publication so that we can do calculations down the road:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9827f6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier\n",
       "206        1879 [1878]\n",
       "216               1868\n",
       "218               1869\n",
       "472               1851\n",
       "480               1857\n",
       "              ...     \n",
       "4158088           1838\n",
       "4158128       1831, 32\n",
       "4159563      [1806]-22\n",
       "4159587           1834\n",
       "4160339        1834-43\n",
       "Name: Date of Publication, Length: 8287, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,'Date of Publication']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d08af2",
   "metadata": {},
   "source": [
    "A particular book can have only one date of publication. Therefore, we need to do the following:\n",
    "\n",
    "- Remove the extra dates in square brackets, wherever present: 1879 [1878]\n",
    "- Convert date ranges to their “start date”, wherever present: 1860-63; 1839, 38-54\n",
    "- Completely remove the dates we are not certain about and replace them with NumPy’s NaN: [1897?]\n",
    "- Convert the string nan to NumPy’s NaN value\n",
    "- Synthesizing these patterns, we can actually take advantage of a single regular expression to extract the publication year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ccbddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this regex pattern\n",
    "regex = r'^(\\d{4})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4ac53",
   "metadata": {},
   "source": [
    "The regular expression above is meant to find any four digits at the beginning of a string, which suffices for our case. The above is a raw string (meaning that a backslash is no longer an escape character), which is standard practice with regular expressions.\n",
    "\n",
    "The \\d represents any digit, and {4} repeats this rule four times. The ^ character matches the start of a string, and the parentheses denote a capturing group, which signals to pandas that we want to extract that part of the regex. (We want ^ to avoid cases where [ starts off the string.)\n",
    "\n",
    "Read more about regular expressions [here](https://realpython.com/regex-python/).\n",
    "\n",
    "Let’s see what happens when we run this regex across our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6e34d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier\n",
       "206        1879\n",
       "216        1868\n",
       "218        1869\n",
       "472        1851\n",
       "480        1857\n",
       "           ... \n",
       "4158088    1838\n",
       "4158128    1831\n",
       "4159563     NaN\n",
       "4159587    1834\n",
       "4160339    1834\n",
       "Name: Date of Publication, Length: 8287, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extr = df['Date of Publication'].str.extract(regex, expand=False)\n",
    "extr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765201b7",
   "metadata": {},
   "source": [
    "Technically, this column still has object dtype, but we can easily get its numerical version with `pd.to_numeric`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7eb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the extracted column to numeric and assign it to the column `Date of Publication`\n",
    "\n",
    "# verify that the type is numeric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ad34c",
   "metadata": {},
   "source": [
    "This results in about one in every ten values being missing, which is a small price to pay for now being able to do computations on the remaining valid values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e2829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the proportion (%) of null values in the newly created date of publication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e976ef",
   "metadata": {},
   "source": [
    "### 1.4 Imputation: Handling missing data\n",
    "You might recall that Date of Publication had some missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89eaa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date of Publication']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba1194",
   "metadata": {},
   "source": [
    "Let's count how many exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eab9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9408e5ac",
   "metadata": {},
   "source": [
    "Missing values can break some functions. Imputation in data science is used to \"fill\" those missing cells with neutral values. Here are some options for imputation: \n",
    "- zero\n",
    "- mean\n",
    "- median\n",
    "\n",
    "Let's start from filling the missing values with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c365aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values with 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275520b2",
   "metadata": {},
   "source": [
    "Choosing the appropriate imputation method is crucial to ensure the data is still meaningful. For instance, filling the Date of Publication missing values with zero might create problems in their interpretation: Does zero means a value was missing or does it mean the book was published in the year 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Date of Publication, dtype: float64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-introduce the nan values\n",
    "df.loc[df[\"Date of Publication\"]==0, \"Date of Publication\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d121c3",
   "metadata": {},
   "source": [
    "A more appropriate method in this case is to simply drop the rows where date of publication is missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where date of publication is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d81b39",
   "metadata": {},
   "source": [
    "### ✏️ Now it's your turn. \n",
    "> Try to replicate what we have done together. \n",
    "> 1. Tidy up the date of publication column using the regex above and convert it to numeric dtype.\n",
    "> 2. Impute the missing values using the median date of publication. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0b2d6",
   "metadata": {},
   "source": [
    "### 1.5 Combining `str` methods with NumPy to clean string columns\n",
    "Above, you may have noticed the use of `df['Date of Publication'].str`. This attribute is a way to access speedy string operations in pandas that largely mimic operations on native Python strings or compiled regular expressions, such as `.split()`, `.replace()`, and `.capitalize()`.\n",
    "\n",
    "To clean the Place of Publication field, we can combine pandas str methods with NumPy’s `np.where` function, which is basically a vectorized form of Excel’s IF() macro. It has the following syntax:\n",
    "\n",
    "`np.where(condition, then, else)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f91e9b6",
   "metadata": {},
   "source": [
    "Here, `condition` is either an array-like object or a Boolean mask. `then` is the value to be used if `condition` evaluates to `True`, and `else` is the value to be used otherwise.\n",
    "\n",
    "Essentially, `.where()` takes each element in the object used for `condition`, checks whether that particular element evaluates to `True` in the context of the `condition`, and returns an `ndarray` containing `then` or `else`, depending on which applies.\n",
    "\n",
    "It can be nested into a compound if-then statement, allowing us to compute values based on multiple conditions:\n",
    "\n",
    "```\n",
    "np.where(condition1, x1, \n",
    "    np.where(condition2, x2, \n",
    "        np.where(condition3, x3, ...)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abde06",
   "metadata": {},
   "source": [
    "We’ll be making use of these two functions to clean Place of Publication since this column has string objects. Here are the contents of the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "954c2b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier\n",
       "206                                  London\n",
       "216                London; Virtue & Yorston\n",
       "218                                  London\n",
       "472                                  London\n",
       "480                                  London\n",
       "481                                  London\n",
       "519                                  London\n",
       "667     pp. 40. G. Bryan & Co: Oxford, 1898\n",
       "874                                 London]\n",
       "1143                                 London\n",
       "Name: Place of Publication, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Place of Publication'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8690870",
   "metadata": {},
   "source": [
    "We see that for some rows, the place of publication is surrounded by other unnecessary information. If we were to look at more values, we would see that this is the case for only some rows that have their place of publication as ‘London’ or ‘Oxford’.\n",
    "\n",
    "Let’s take a look at two specific entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f5241f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place of Publication                                  Newcastle-upon-Tyne\n",
       "Date of Publication                                                1867.0\n",
       "Publisher                                                      T. Fordyce\n",
       "Title                   Local Records; or, Historical Register of rema...\n",
       "Author                      FORDYCE, T. - Printer, of Newcastle-upon-Tyne\n",
       "Flickr URL              http://www.flickr.com/photos/britishlibrary/ta...\n",
       "Name: 4157862, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[4157862]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "769cf0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place of Publication                                  Newcastle upon Tyne\n",
       "Date of Publication                                                1834.0\n",
       "Publisher                                                Mackenzie & Dent\n",
       "Title                   An historical, topographical and descriptive v...\n",
       "Author                                              Mackenzie, E. (Eneas)\n",
       "Flickr URL              http://www.flickr.com/photos/britishlibrary/ta...\n",
       "Name: 4159587, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[4159587]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b97a7",
   "metadata": {},
   "source": [
    "These two books were published in the same place, but one has hyphens in the name of the place while the other does not.\n",
    "\n",
    "To clean this column in one sweep, we can use str.contains() to get a Boolean mask.\n",
    "\n",
    "We clean the column as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "69a46d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identifier\n",
       "206    True\n",
       "216    True\n",
       "218    True\n",
       "472    True\n",
       "480    True\n",
       "Name: Place of Publication, dtype: bool"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub = df['Place of Publication']\n",
    "london = pub.str.contains('London')\n",
    "london[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c522a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford = pub.str.contains('Oxford')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d33560",
   "metadata": {},
   "source": [
    "We combine them with `np.where`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ce8caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Place of Publication'] = np.where(london, 'London',\n",
    "                                      np.where(oxford, 'Oxford',\n",
    "                                               pub.str.replace('-', ' ')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b181e",
   "metadata": {},
   "source": [
    "Here, the np.where function is called in a nested structure, with condition being a Series of Booleans obtained with str.contains(). The contains() method works similarly to the built-in in keyword used to find the occurrence of an entity in an iterable (or substring in a string).\n",
    "\n",
    "The replacement to be used is a string representing our desired place of publication. We also replace hyphens with a space with str.replace() and reassign to the column in our DataFrame.\n",
    "\n",
    "Although there is more dirty data in this dataset, we will discuss only these two columns for now.\n",
    "\n",
    "Let’s have a look at the first five entries, which look a lot crisper than when we started out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f557ea",
   "metadata": {},
   "source": [
    "Things are starting to look cleaner! But let's see if we can improve things furter. \n",
    "\n",
    "Do you know what does the `unique` method do? If not Google it! The apply it here to learn about the places of publication in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292c8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a6c08",
   "metadata": {},
   "source": [
    "### ✏️ Now it's your turn. \n",
    "> 'Москва, 1860' and 'Moskwa' (i.e. Moscow in English) seem redundant. Apply a str method of your choice among those reviewd above to replace the with the english version 'Moscow'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a309e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d9373b0",
   "metadata": {},
   "source": [
    "# Olympics dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f45ff5",
   "metadata": {},
   "source": [
    "### 1.6 Renaming Columns and Skipping Rows\n",
    "Often, the datasets you’ll work with will have either column names that are not easy to understand, or unimportant information in the first few and/or last rows, such as definitions of the terms in the dataset, or footnotes.\n",
    "\n",
    "In that case, we’d want to rename columns and skip certain rows so that we can drill down to necessary information with correct and sensible labels.\n",
    "\n",
    "To demonstrate how we can go about doing this, let’s first take a glance at the initial five rows of the “olympics.csv” dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34710fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870fa578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the olympics dataset into a dataframe\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/realpython/python-data-cleaning/refs/heads/master/Datasets/olympics.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad885bc",
   "metadata": {},
   "source": [
    "This is messy indeed! The columns are the string form of integers indexed at 0. The row which should have been our header (i.e. the one to be used to set the column names) is at `olympics_df.iloc[0]`. This happened because our CSV file starts with 0, 1, 2, …, 15.\n",
    "\n",
    "Also, if we were to go to the source of this dataset, we’d see that `NaN` above should really be something like “Country”, ? Summer is supposed to represent “Summer Games”, 01 ! should be “Gold”, and so on.\n",
    "\n",
    "Therefore, we need to do two things:\n",
    "\n",
    "- Skip one row and set the header as the first (0-indexed) row\n",
    "- Rename the columns\n",
    "We can skip rows and set the header while reading the CSV file by passing some parameters to the `read_csv()` function.\n",
    "\n",
    "This function takes a lot of optional parameters, but in this case we only need one (header) to remove the 0th row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "32c911b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>? Summer</th>\n",
       "      <th>01 !</th>\n",
       "      <th>02 !</th>\n",
       "      <th>03 !</th>\n",
       "      <th>Total</th>\n",
       "      <th>? Winter</th>\n",
       "      <th>01 !.1</th>\n",
       "      <th>02 !.1</th>\n",
       "      <th>03 !.1</th>\n",
       "      <th>Total.1</th>\n",
       "      <th>? Games</th>\n",
       "      <th>01 !.2</th>\n",
       "      <th>02 !.2</th>\n",
       "      <th>03 !.2</th>\n",
       "      <th>Combined total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan (AFG)</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria (ALG)</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Argentina (ARG)</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>70</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Armenia (ARM)</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australasia (ANZ) [ANZ]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Unnamed: 0  ? Summer  01 !  02 !  03 !  Total  ? Winter  \\\n",
       "0        Afghanistan (AFG)        13     0     0     2      2         0   \n",
       "1            Algeria (ALG)        12     5     2     8     15         3   \n",
       "2          Argentina (ARG)        23    18    24    28     70        18   \n",
       "3            Armenia (ARM)         5     1     2     9     12         6   \n",
       "4  Australasia (ANZ) [ANZ]         2     3     4     5     12         0   \n",
       "\n",
       "   01 !.1  02 !.1  03 !.1  Total.1  ? Games  01 !.2  02 !.2  03 !.2  \\\n",
       "0       0       0       0        0       13       0       0       2   \n",
       "1       0       0       0        0       15       5       2       8   \n",
       "2       0       0       0        0       41      18      24      28   \n",
       "3       0       0       0        0       11       1       2       9   \n",
       "4       0       0       0        0        2       3       4       5   \n",
       "\n",
       "   Combined total  \n",
       "0               2  \n",
       "1              15  \n",
       "2              70  \n",
       "3              12  \n",
       "4              12  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olympics_df = pd.read_csv('https://raw.githubusercontent.com/realpython/python-data-cleaning/refs/heads/master/Datasets/olympics.csv', header=1)\n",
    "olympics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989e541",
   "metadata": {},
   "source": [
    "We now have the correct row set as the header and all unnecessary rows removed. Take note of how pandas has changed the name of the column containing the name of the countries from NaN to Unnamed: 0.\n",
    "\n",
    "To rename the columns, we will make use of a DataFrame’s rename() method, which allows you to relabel an axis based on a mapping (in this case, a dict).\n",
    "\n",
    "Let’s start by defining a dictionary that maps current column names (as keys) to more usable ones (the dictionary’s values):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that maps old column names to new ones\n",
    "\n",
    "\n",
    "# rename the columns using .rename()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abfb5f",
   "metadata": {},
   "source": [
    "We call the `.rename()` function on our object:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf855f4c",
   "metadata": {},
   "source": [
    "Setting inplace to True specifies that our changes be made directly to the object. Let’s see if this checks out:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c25c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "720fc05b",
   "metadata": {},
   "source": [
    "### ✏️ Now it's your turn. \n",
    "> Read in the olympics dataset and rename the columns as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02839cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
